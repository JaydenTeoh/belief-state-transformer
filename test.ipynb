{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Model\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "model = GPT2Model.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124439808"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params  = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaydenteoh/Library/Python/3.9/lib/python/site-packages/peft/tuners/lora/layer.py:1119: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class TiedTextHead(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, vocab_size, tied_weights=None):\n",
    "        super().__init__()\n",
    "        self.shared_mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # output is twice vocab size\n",
    "        # first half is for next token prediction: x_{t+1}\n",
    "        # second half is for previous token prediction: x_{t+k-1}\n",
    "        self.output_layer = nn.Linear(hidden_size, vocab_size * 2)\n",
    "\n",
    "    def forward(self, f, b):\n",
    "        combined = torch.cat([f, b], dim=-1)\n",
    "        shared_output = self.shared_mlp(combined)\n",
    "        logits = self.output_layer(shared_output)\n",
    "        return logits\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "            r=16, \n",
    "            lora_alpha=32, \n",
    "            lora_dropout=0.05, \n",
    "            # target_modules=[\"q_proj\", \"v_proj\"],  # apply lora to attention layers\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "\n",
    "# create separate forward and backward lora adapters\n",
    "model.add_adapter(lora_config, adapter_name=\"forward_encoder\")\n",
    "model.add_adapter(lora_config, adapter_name=\"backward_encoder\")\n",
    "\n",
    "# add tied text head for next and previous token predictions\n",
    "text_head = TiedTextHead(\n",
    "                    input_dim=model.config.hidden_size * 2,\n",
    "                    hidden_size=512, # TODO; allow this to be configurable\n",
    "                    vocab_size=50257,\n",
    "                    # tied_weights=self.model.transformer.wte.weight  # use input embeddings' weights\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "589824"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params  = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from utils.training_utils import accuracy\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class TiedTextHead(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, vocab_size, tied_weights=None):\n",
    "        super().__init__()\n",
    "        self.shared_mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        # output is twice vocab size\n",
    "        # first half is for next token prediction: x_{t+1}\n",
    "        # second half is for previous token prediction: x_{t+k-1}\n",
    "        self.output_layer = nn.Linear(hidden_size, vocab_size * 2)\n",
    "\n",
    "    def forward(self, f, b):\n",
    "        combined = torch.cat([f, b], dim=-1)\n",
    "        shared_output = self.shared_mlp(combined)\n",
    "        logits = self.output_layer(shared_output)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class BeliefStateTransformer(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "\n",
    "        if args.load_in_4bit:\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                                    load_in_4bit=True,\n",
    "                                    bnb_4bit_compute_dtype=args.ptdtype\n",
    "                                )\n",
    "        else:\n",
    "            # Assume default or no quantization\n",
    "            quantization_config = None\n",
    "\n",
    "        if args.use_flash:\n",
    "            attn_implementation = \"flash_attention_2\"\n",
    "        else:\n",
    "            attn_implementation = None\n",
    "        \n",
    "        # TODO: support more pretrained models\n",
    "        # import gpt2 with no specific head on top\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            args.model, \n",
    "            attn_implementation=attn_implementation,\n",
    "            torch_dtype=args.ptdtype,\n",
    "            quantization_config=quantization_config\n",
    "        )\n",
    "\n",
    "        # lora adapter config\n",
    "        lora_config = LoraConfig(\n",
    "            r=args.lora_r, \n",
    "            lora_alpha=args.lora_alpha, \n",
    "            lora_dropout=args.lora_dropout, \n",
    "            # target_modules=[\"q_proj\", \"v_proj\"],  # apply lora to attention layers\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "            fan_in_fan_out=True\n",
    "        )\n",
    "\n",
    "        # create separate forward and backward lora adapters\n",
    "        self.model.add_adapter(lora_config, adapter_name=\"forward_encoder\")\n",
    "        self.model.add_adapter(lora_config, adapter_name=\"backward_encoder\")\n",
    "\n",
    "        # enable gradient checkpointing to save memory during training\n",
    "        self.model.gradient_checkpointing_enable()\n",
    "\n",
    "        # add tied text head for next and previous token predictions\n",
    "        self.text_head = TiedTextHead(\n",
    "                            input_dim=self.model.config.hidden_size * 2,\n",
    "                            hidden_size=512, # TODO; allow this to be configurable\n",
    "                            vocab_size=args.vocab_size,\n",
    "                            # tied_weights=self.model.transformer.wte.weight  # use input embeddings' weights\n",
    "                        )\n",
    "\n",
    "    def forward(self, f, b):\n",
    "        # forward encoding\n",
    "        self.model.set_adapter(\"forward_encoder\")\n",
    "        forward_states = self.model(f).last_hidden_state  # get forward states\n",
    "\n",
    "        # backward encoding\n",
    "        self.model.set_adapter(\"backward_encoder\")\n",
    "        backward_input = torch.flip(b, dims=[1])  # reverse the input sequence\n",
    "        backward_states = self.model(backward_input).last_hidden_state\n",
    "        backward_states = torch.flip(backward_states, dims=[1])  # flip the backward states back\n",
    "\n",
    "        # Text head for next and previous token predictions\n",
    "        next_logits, prev_logits = self.text_head(forward_states, backward_states)\n",
    "        # acc, token_acc = accuracy(logits, targets)\n",
    "        # accs = {\"acc\": acc, \"token_acc\": token_acc}\n",
    "        return next_logits, prev_logits\n",
    "    \n",
    "    def belief_state_objective(self, all_f, all_b, x):\n",
    "        \"\"\"\n",
    "        Compute the belief state objective as described in the BST paper.\n",
    "        \"\"\"\n",
    "        bs, T = x.shape\n",
    "        forward_states = all_f\n",
    "        backward_states = all_b.flip(1)\n",
    "        # generate all valid combinations of forward and backward indices\n",
    "        ft = torch.arange(T, dtype=torch.int32)  # forward indices\n",
    "        bt = torch.arange(T, dtype=torch.int32)  # backward indices\n",
    "        combinations = torch.cartesian_prod(ft, bt) \n",
    "        combinations = combinations[(combinations[:, 1] - combinations[:, 0] >= 2)]  # filter valid pairs\n",
    "        fb_pairs = combinations[combinations[:, 1] < T]  # ensure backward indices are within range\n",
    "\n",
    "        # extract valid indices\n",
    "        f_idxs, b_idxs = fb_pairs[:, 0], fb_pairs[:, 1]\n",
    "        nt_idxs = (combinations[:, 0] + 1)  # indices for next token labels\n",
    "\n",
    "        # gather forward and backward features\n",
    "        f = forward_states[:, f_idxs]\n",
    "        b = backward_states[:, b_idxs]\n",
    "\n",
    "        # prepare labels\n",
    "        single_labels_f = x[:, nt_idxs].unsqueeze(2)  # labels for next-token prediction\n",
    "        single_labels_b = x[:, b_idxs].unsqueeze(2)   # labels for prev-token prediction\n",
    "        single_labels = torch.cat((single_labels_f, single_labels_b), dim=2)\n",
    "\n",
    "        # compute logits from the text head\n",
    "        logits = self.text_head(f, b)  # combine forward and backward states\n",
    "        fb_numpairs = fb_pairs.shape[0]  # no of valid forward-backward pairs\n",
    "\n",
    "        # reshape logits and labels for loss computation\n",
    "        logits = logits.reshape((bs, fb_numpairs, 2, -1))  # split into next and previous logits\n",
    "        logits = logits.reshape((bs * fb_numpairs * 2, -1))  # flatten for CEL\n",
    "        single_labels = single_labels.reshape((bs * fb_numpairs * 2))  # flatten labels\n",
    "\n",
    "        # compute the loss independently for next and previous token predictions\n",
    "        # this also sums the negative log likelihood for \n",
    "        # all next and previous token predictions together, aligning with the paper\n",
    "        loss = nn.CrossEntropyLoss()(logits, single_labels)\n",
    "        return loss\n",
    "    \n",
    "    def update(self, x, optimizer, scaler, targets=None):\n",
    "        \"\"\"\n",
    "        Efficient training for the BST model.\n",
    "        \"\"\"\n",
    "        # Compute forward states\n",
    "        self.model.set_adapter(\"forward_encoder\")\n",
    "        forward_states = self.model(x).last_hidden_state\n",
    "\n",
    "        # Compute backward states\n",
    "        self.model.set_adapter(\"backward_encoder\")\n",
    "        backward_input = torch.flip(x, dims=[1])\n",
    "        backward_states = self.model(backward_input).last_hidden_state\n",
    "\n",
    "        # Compute the combined loss\n",
    "        loss = self.belief_state_objective(forward_states, backward_states, x)\n",
    "        self.model.set_adapter([\"forward_encoder\", \"backward_encoder\"]) # IMPORTANT else gradient will only exist for backward_encoder\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=1):\n",
    "        \"\"\"\n",
    "        Generate text autoregressively using the forward model.\n",
    "\n",
    "        Args:\n",
    "            idx (torch.Tensor): Input tensor containing the tokenized prefix (batch_size, sequence_length).\n",
    "            max_length (int): Maximum length of the generated sequence.\n",
    "            temperature (float): Sampling temperature.\n",
    "            top_k (int): Top-k sampling.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Tokenized output sequence.\n",
    "        \"\"\"\n",
    "        bsz, prefix_len = idx.shape\n",
    "        device = idx.device\n",
    "\n",
    "        # Step 1: Precompute backward latent state for empty suffix B(âˆ…)\n",
    "        empty_suffix = torch.zeros((bsz, 1), dtype=idx.dtype, device=device)\n",
    "        self.model.set_adapter(\"backward_encoder\")\n",
    "        backward_state = self.model(empty_suffix).last_hidden_state[:, 0:1, :]  # Fixed backward state\n",
    "\n",
    "        # Step 2: Initialize the generated sequence with the prefix\n",
    "        out = idx.clone()\n",
    "\n",
    "        for i in range(max_new_tokens):\n",
    "            # Step 3: Compute forward latent state for current prefix F(x1:t)\n",
    "            self.model.set_adapter(\"forward_encoder\")\n",
    "            forward_states = self.model(out).last_hidden_state\n",
    "\n",
    "            # Step 4: Compute logits for the next token prediction\n",
    "            logits = self.text_head(forward_states[:, -1:, :], backward_state)\n",
    "\n",
    "            next_logits = logits[:, 0, :self.text_head.output_layer.out_features // 2]\n",
    "\n",
    "            # Step 5: Apply temperature scaling\n",
    "            next_logits = next_logits / temperature\n",
    "\n",
    "            # Step 6: Apply top-k sampling if specified\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(next_logits, min(top_k, next_logits.size(-1)))\n",
    "                next_logits[next_logits < v[:, [-1]]] = -float('Inf')\n",
    "\n",
    "            # Step 7: Convert logits to probabilities and sample\n",
    "            probs = F.softmax(next_logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            # Step 8: Append sampled index to the running sequence\n",
    "            out = torch.cat((out, idx_next), dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    \n",
    "    # def _create_optimizer(self, lr, weight_decay):\n",
    "    #     return torch.optim.AdamW(\n",
    "    #         [\n",
    "    #             {\"params\": self.model.get_adapter(\"forward_encoder\").parameters(), \"lr\": lr},\n",
    "    #             {\"params\": self.model.get_adapter(\"backward_encoder\").parameters(), \"lr\": lr},\n",
    "    #             {\"params\": self.text_head.parameters(), \"lr\": lr}\n",
    "    #         ],\n",
    "    #         lr=lr,\n",
    "    #         weight_decay=weight_decay\n",
    "    #     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaydenteoh/Library/Python/3.9/lib/python/site-packages/peft/tuners/lora/layer.py:1110: UserWarning: fan_in_fan_out is set to True but the target module is `torch.nn.Linear`. Setting fan_in_fan_out to False.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 10.324504852294922\n",
      "\n",
      "Parameter norms before optimizer step:\n",
      "model.encoder.layer.0.attention.self.query.lora_A.forward_encoder.weight: 1.155055046081543\n",
      "model.encoder.layer.0.attention.self.query.lora_A.backward_encoder.weight: 1.1544753313064575\n",
      "model.encoder.layer.0.attention.self.query.lora_B.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.0.attention.self.query.lora_B.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.0.attention.self.value.lora_A.forward_encoder.weight: 1.156412124633789\n",
      "model.encoder.layer.0.attention.self.value.lora_A.backward_encoder.weight: 1.1613956689834595\n",
      "model.encoder.layer.0.attention.self.value.lora_B.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.0.attention.self.value.lora_B.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.1.attention.self.query.lora_A.forward_encoder.weight: 1.1700341701507568\n",
      "model.encoder.layer.1.attention.self.query.lora_A.backward_encoder.weight: 1.1603801250457764\n",
      "model.encoder.layer.1.attention.self.query.lora_B.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.1.attention.self.query.lora_B.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.1.attention.self.value.lora_A.forward_encoder.weight: 1.1583845615386963\n",
      "model.encoder.layer.1.attention.self.value.lora_A.backward_encoder.weight: 1.1555118560791016\n",
      "model.encoder.layer.1.attention.self.value.lora_B.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.1.attention.self.value.lora_B.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.2.attention.self.query.lora_A.forward_encoder.weight: 1.1403778791427612\n",
      "model.encoder.layer.2.attention.self.query.lora_A.backward_encoder.weight: 1.1532394886016846\n",
      "model.encoder.layer.2.attention.self.query.lora_B.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.2.attention.self.query.lora_B.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.2.attention.self.value.lora_A.forward_encoder.weight: 1.1443812847137451\n",
      "model.encoder.layer.2.attention.self.value.lora_A.backward_encoder.weight: 1.1614967584609985\n",
      "model.encoder.layer.2.attention.self.value.lora_B.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.2.attention.self.value.lora_B.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.3.attention.self.query.lora_A.forward_encoder.weight: 1.1493151187896729\n",
      "model.encoder.layer.3.attention.self.query.lora_A.backward_encoder.weight: 1.1552228927612305\n",
      "model.encoder.layer.3.attention.self.query.lora_B.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.3.attention.self.query.lora_B.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.3.attention.self.value.lora_A.forward_encoder.weight: 1.1532777547836304\n",
      "model.encoder.layer.3.attention.self.value.lora_A.backward_encoder.weight: 1.1466686725616455\n",
      "model.encoder.layer.3.attention.self.value.lora_B.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.3.attention.self.value.lora_B.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.4.attention.self.query.lora_A.forward_encoder.weight: 1.1635855436325073\n",
      "model.encoder.layer.4.attention.self.query.lora_A.backward_encoder.weight: 1.144288420677185\n",
      "model.encoder.layer.4.attention.self.query.lora_B.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.4.attention.self.query.lora_B.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.4.attention.self.value.lora_A.forward_encoder.weight: 1.1478371620178223\n",
      "model.encoder.layer.4.attention.self.value.lora_A.backward_encoder.weight: 1.1657867431640625\n",
      "model.encoder.layer.4.attention.self.value.lora_B.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.4.attention.self.value.lora_B.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.5.attention.self.query.lora_A.forward_encoder.weight: 1.1510975360870361\n",
      "model.encoder.layer.5.attention.self.query.lora_A.backward_encoder.weight: 1.1520044803619385\n",
      "model.encoder.layer.5.attention.self.query.lora_B.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.5.attention.self.query.lora_B.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.5.attention.self.value.lora_A.forward_encoder.weight: 1.1517558097839355\n",
      "model.encoder.layer.5.attention.self.value.lora_A.backward_encoder.weight: 1.155048131942749\n",
      "model.encoder.layer.5.attention.self.value.lora_B.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.5.attention.self.value.lora_B.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.6.attention.self.query.lora_A.forward_encoder.weight: 1.1522637605667114\n",
      "model.encoder.layer.6.attention.self.query.lora_A.backward_encoder.weight: 1.1442995071411133\n",
      "model.encoder.layer.6.attention.self.query.lora_B.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.6.attention.self.query.lora_B.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.6.attention.self.value.lora_A.forward_encoder.weight: 1.1562122106552124\n",
      "model.encoder.layer.6.attention.self.value.lora_A.backward_encoder.weight: 1.168866515159607\n",
      "model.encoder.layer.6.attention.self.value.lora_B.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.6.attention.self.value.lora_B.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.7.attention.self.query.lora_A.forward_encoder.weight: 1.1536980867385864\n",
      "model.encoder.layer.7.attention.self.query.lora_A.backward_encoder.weight: 1.1474555730819702\n",
      "model.encoder.layer.7.attention.self.query.lora_B.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.7.attention.self.query.lora_B.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.7.attention.self.value.lora_A.forward_encoder.weight: 1.1683076620101929\n",
      "model.encoder.layer.7.attention.self.value.lora_A.backward_encoder.weight: 1.150134801864624\n",
      "model.encoder.layer.7.attention.self.value.lora_B.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.7.attention.self.value.lora_B.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.8.attention.self.query.lora_A.forward_encoder.weight: 1.147923469543457\n",
      "model.encoder.layer.8.attention.self.query.lora_A.backward_encoder.weight: 1.1592763662338257\n",
      "model.encoder.layer.8.attention.self.query.lora_B.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.8.attention.self.query.lora_B.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.8.attention.self.value.lora_A.forward_encoder.weight: 1.1513773202896118\n",
      "model.encoder.layer.8.attention.self.value.lora_A.backward_encoder.weight: 1.1591681241989136\n",
      "model.encoder.layer.8.attention.self.value.lora_B.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.8.attention.self.value.lora_B.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.9.attention.self.query.lora_A.forward_encoder.weight: 1.1614500284194946\n",
      "model.encoder.layer.9.attention.self.query.lora_A.backward_encoder.weight: 1.151311993598938\n",
      "model.encoder.layer.9.attention.self.query.lora_B.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.9.attention.self.query.lora_B.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.9.attention.self.value.lora_A.forward_encoder.weight: 1.1555498838424683\n",
      "model.encoder.layer.9.attention.self.value.lora_A.backward_encoder.weight: 1.1524509191513062\n",
      "model.encoder.layer.9.attention.self.value.lora_B.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.9.attention.self.value.lora_B.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.10.attention.self.query.lora_A.forward_encoder.weight: 1.1570909023284912\n",
      "model.encoder.layer.10.attention.self.query.lora_A.backward_encoder.weight: 1.177747368812561\n",
      "model.encoder.layer.10.attention.self.query.lora_B.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.10.attention.self.query.lora_B.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.10.attention.self.value.lora_A.forward_encoder.weight: 1.1486409902572632\n",
      "model.encoder.layer.10.attention.self.value.lora_A.backward_encoder.weight: 1.15969717502594\n",
      "model.encoder.layer.10.attention.self.value.lora_B.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.10.attention.self.value.lora_B.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.11.attention.self.query.lora_A.forward_encoder.weight: 1.1651676893234253\n",
      "model.encoder.layer.11.attention.self.query.lora_A.backward_encoder.weight: 1.153135895729065\n",
      "model.encoder.layer.11.attention.self.query.lora_B.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.11.attention.self.query.lora_B.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.11.attention.self.value.lora_A.forward_encoder.weight: 1.162435531616211\n",
      "model.encoder.layer.11.attention.self.value.lora_A.backward_encoder.weight: 1.1421624422073364\n",
      "model.encoder.layer.11.attention.self.value.lora_B.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.11.attention.self.value.lora_B.backward_encoder.weight: 0.0\n",
      "\n",
      "Parameter norms after optimizer step:\n",
      "model.encoder.layer.0.attention.self.query.lora_A.forward_encoder.weight: 1.1550434827804565, Change: -1.1563301086425781e-05\n",
      "model.encoder.layer.0.attention.self.query.lora_A.backward_encoder.weight: 1.154463768005371, Change: -1.1563301086425781e-05\n",
      "model.encoder.layer.0.attention.self.query.lora_B.forward_encoder.weight: 0.05537842586636543, Change: 0.05537842586636543\n",
      "model.encoder.layer.0.attention.self.query.lora_B.backward_encoder.weight: 0.05541029945015907, Change: 0.05541029945015907\n",
      "model.encoder.layer.0.attention.self.value.lora_A.forward_encoder.weight: 1.1564005613327026, Change: -1.1563301086425781e-05\n",
      "model.encoder.layer.0.attention.self.value.lora_A.backward_encoder.weight: 1.161384105682373, Change: -1.1563301086425781e-05\n",
      "model.encoder.layer.0.attention.self.value.lora_B.forward_encoder.weight: 0.05541052296757698, Change: 0.05541052296757698\n",
      "model.encoder.layer.0.attention.self.value.lora_B.backward_encoder.weight: 0.05541960150003433, Change: 0.05541960150003433\n",
      "model.encoder.layer.1.attention.self.query.lora_A.forward_encoder.weight: 1.1700224876403809, Change: -1.1682510375976562e-05\n",
      "model.encoder.layer.1.attention.self.query.lora_A.backward_encoder.weight: 1.1603684425354004, Change: -1.1682510375976562e-05\n",
      "model.encoder.layer.1.attention.self.query.lora_B.forward_encoder.weight: 0.05536641180515289, Change: 0.05536641180515289\n",
      "model.encoder.layer.1.attention.self.query.lora_B.backward_encoder.weight: 0.05540936440229416, Change: 0.05540936440229416\n",
      "model.encoder.layer.1.attention.self.value.lora_A.forward_encoder.weight: 1.1583729982376099, Change: -1.1563301086425781e-05\n",
      "model.encoder.layer.1.attention.self.value.lora_A.backward_encoder.weight: 1.1555001735687256, Change: -1.1682510375976562e-05\n",
      "model.encoder.layer.1.attention.self.value.lora_B.forward_encoder.weight: 0.05541450157761574, Change: 0.05541450157761574\n",
      "model.encoder.layer.1.attention.self.value.lora_B.backward_encoder.weight: 0.05542198568582535, Change: 0.05542198568582535\n",
      "model.encoder.layer.2.attention.self.query.lora_A.forward_encoder.weight: 1.140366554260254, Change: -1.1324882507324219e-05\n",
      "model.encoder.layer.2.attention.self.query.lora_A.backward_encoder.weight: 1.1532279253005981, Change: -1.1563301086425781e-05\n",
      "model.encoder.layer.2.attention.self.query.lora_B.forward_encoder.weight: 0.0553324818611145, Change: 0.0553324818611145\n",
      "model.encoder.layer.2.attention.self.query.lora_B.backward_encoder.weight: 0.055308595299720764, Change: 0.055308595299720764\n",
      "model.encoder.layer.2.attention.self.value.lora_A.forward_encoder.weight: 1.1443698406219482, Change: -1.1444091796875e-05\n",
      "model.encoder.layer.2.attention.self.value.lora_A.backward_encoder.weight: 1.161484956741333, Change: -1.1801719665527344e-05\n",
      "model.encoder.layer.2.attention.self.value.lora_B.forward_encoder.weight: 0.05541094392538071, Change: 0.05541094392538071\n",
      "model.encoder.layer.2.attention.self.value.lora_B.backward_encoder.weight: 0.05542244017124176, Change: 0.05542244017124176\n",
      "model.encoder.layer.3.attention.self.query.lora_A.forward_encoder.weight: 1.1493035554885864, Change: -1.1563301086425781e-05\n",
      "model.encoder.layer.3.attention.self.query.lora_A.backward_encoder.weight: 1.155211329460144, Change: -1.1563301086425781e-05\n",
      "model.encoder.layer.3.attention.self.query.lora_B.forward_encoder.weight: 0.05538061261177063, Change: 0.05538061261177063\n",
      "model.encoder.layer.3.attention.self.query.lora_B.backward_encoder.weight: 0.05540534481406212, Change: 0.05540534481406212\n",
      "model.encoder.layer.3.attention.self.value.lora_A.forward_encoder.weight: 1.1532663106918335, Change: -1.1444091796875e-05\n",
      "model.encoder.layer.3.attention.self.value.lora_A.backward_encoder.weight: 1.1466572284698486, Change: -1.1444091796875e-05\n",
      "model.encoder.layer.3.attention.self.value.lora_B.forward_encoder.weight: 0.05541687086224556, Change: 0.05541687086224556\n",
      "model.encoder.layer.3.attention.self.value.lora_B.backward_encoder.weight: 0.055416468530893326, Change: 0.055416468530893326\n",
      "model.encoder.layer.4.attention.self.query.lora_A.forward_encoder.weight: 1.1635738611221313, Change: -1.1682510375976562e-05\n",
      "model.encoder.layer.4.attention.self.query.lora_A.backward_encoder.weight: 1.1442768573760986, Change: -1.1563301086425781e-05\n",
      "model.encoder.layer.4.attention.self.query.lora_B.forward_encoder.weight: 0.055387962609529495, Change: 0.055387962609529495\n",
      "model.encoder.layer.4.attention.self.query.lora_B.backward_encoder.weight: 0.05539480596780777, Change: 0.05539480596780777\n",
      "model.encoder.layer.4.attention.self.value.lora_A.forward_encoder.weight: 1.1478254795074463, Change: -1.1682510375976562e-05\n",
      "model.encoder.layer.4.attention.self.value.lora_A.backward_encoder.weight: 1.165775179862976, Change: -1.1563301086425781e-05\n",
      "model.encoder.layer.4.attention.self.value.lora_B.forward_encoder.weight: 0.05541788041591644, Change: 0.05541788041591644\n",
      "model.encoder.layer.4.attention.self.value.lora_B.backward_encoder.weight: 0.055421583354473114, Change: 0.055421583354473114\n",
      "model.encoder.layer.5.attention.self.query.lora_A.forward_encoder.weight: 1.1510860919952393, Change: -1.1444091796875e-05\n",
      "model.encoder.layer.5.attention.self.query.lora_A.backward_encoder.weight: 1.151992917060852, Change: -1.1563301086425781e-05\n",
      "model.encoder.layer.5.attention.self.query.lora_B.forward_encoder.weight: 0.05537218227982521, Change: 0.05537218227982521\n",
      "model.encoder.layer.5.attention.self.query.lora_B.backward_encoder.weight: 0.05538995563983917, Change: 0.05538995563983917\n",
      "model.encoder.layer.5.attention.self.value.lora_A.forward_encoder.weight: 1.1517443656921387, Change: -1.1444091796875e-05\n",
      "model.encoder.layer.5.attention.self.value.lora_A.backward_encoder.weight: 1.155036449432373, Change: -1.1682510375976562e-05\n",
      "model.encoder.layer.5.attention.self.value.lora_B.forward_encoder.weight: 0.055417198687791824, Change: 0.055417198687791824\n",
      "model.encoder.layer.5.attention.self.value.lora_B.backward_encoder.weight: 0.0554187037050724, Change: 0.0554187037050724\n",
      "model.encoder.layer.6.attention.self.query.lora_A.forward_encoder.weight: 1.152252197265625, Change: -1.1563301086425781e-05\n",
      "model.encoder.layer.6.attention.self.query.lora_A.backward_encoder.weight: 1.1442878246307373, Change: -1.1682510375976562e-05\n",
      "model.encoder.layer.6.attention.self.query.lora_B.forward_encoder.weight: 0.05535273626446724, Change: 0.05535273626446724\n",
      "model.encoder.layer.6.attention.self.query.lora_B.backward_encoder.weight: 0.0553537979722023, Change: 0.0553537979722023\n",
      "model.encoder.layer.6.attention.self.value.lora_A.forward_encoder.weight: 1.156200647354126, Change: -1.1563301086425781e-05\n",
      "model.encoder.layer.6.attention.self.value.lora_A.backward_encoder.weight: 1.168854832649231, Change: -1.1682510375976562e-05\n",
      "model.encoder.layer.6.attention.self.value.lora_B.forward_encoder.weight: 0.05541243031620979, Change: 0.05541243031620979\n",
      "model.encoder.layer.6.attention.self.value.lora_B.backward_encoder.weight: 0.05541818216443062, Change: 0.05541818216443062\n",
      "model.encoder.layer.7.attention.self.query.lora_A.forward_encoder.weight: 1.1536866426467896, Change: -1.1444091796875e-05\n",
      "model.encoder.layer.7.attention.self.query.lora_A.backward_encoder.weight: 1.1474441289901733, Change: -1.1444091796875e-05\n",
      "model.encoder.layer.7.attention.self.query.lora_B.forward_encoder.weight: 0.055341657251119614, Change: 0.055341657251119614\n",
      "model.encoder.layer.7.attention.self.query.lora_B.backward_encoder.weight: 0.05534078925848007, Change: 0.05534078925848007\n",
      "model.encoder.layer.7.attention.self.value.lora_A.forward_encoder.weight: 1.1682960987091064, Change: -1.1563301086425781e-05\n",
      "model.encoder.layer.7.attention.self.value.lora_A.backward_encoder.weight: 1.1501232385635376, Change: -1.1563301086425781e-05\n",
      "model.encoder.layer.7.attention.self.value.lora_B.forward_encoder.weight: 0.05541572347283363, Change: 0.05541572347283363\n",
      "model.encoder.layer.7.attention.self.value.lora_B.backward_encoder.weight: 0.05542036145925522, Change: 0.05542036145925522\n",
      "model.encoder.layer.8.attention.self.query.lora_A.forward_encoder.weight: 1.1479119062423706, Change: -1.1563301086425781e-05\n",
      "model.encoder.layer.8.attention.self.query.lora_A.backward_encoder.weight: 1.1592646837234497, Change: -1.1682510375976562e-05\n",
      "model.encoder.layer.8.attention.self.query.lora_B.forward_encoder.weight: 0.055346909910440445, Change: 0.055346909910440445\n",
      "model.encoder.layer.8.attention.self.query.lora_B.backward_encoder.weight: 0.0553559772670269, Change: 0.0553559772670269\n",
      "model.encoder.layer.8.attention.self.value.lora_A.forward_encoder.weight: 1.1513657569885254, Change: -1.1563301086425781e-05\n",
      "model.encoder.layer.8.attention.self.value.lora_A.backward_encoder.weight: 1.159156322479248, Change: -1.1801719665527344e-05\n",
      "model.encoder.layer.8.attention.self.value.lora_B.forward_encoder.weight: 0.05541444942355156, Change: 0.05541444942355156\n",
      "model.encoder.layer.8.attention.self.value.lora_B.backward_encoder.weight: 0.05541331693530083, Change: 0.05541331693530083\n",
      "model.encoder.layer.9.attention.self.query.lora_A.forward_encoder.weight: 1.1614384651184082, Change: -1.1563301086425781e-05\n",
      "model.encoder.layer.9.attention.self.query.lora_A.backward_encoder.weight: 1.1513006687164307, Change: -1.1324882507324219e-05\n",
      "model.encoder.layer.9.attention.self.query.lora_B.forward_encoder.weight: 0.055378444492816925, Change: 0.055378444492816925\n",
      "model.encoder.layer.9.attention.self.query.lora_B.backward_encoder.weight: 0.055338967591524124, Change: 0.055338967591524124\n",
      "model.encoder.layer.9.attention.self.value.lora_A.forward_encoder.weight: 1.1555384397506714, Change: -1.1444091796875e-05\n",
      "model.encoder.layer.9.attention.self.value.lora_A.backward_encoder.weight: 1.1524392366409302, Change: -1.1682510375976562e-05\n",
      "model.encoder.layer.9.attention.self.value.lora_B.forward_encoder.weight: 0.055418942123651505, Change: 0.055418942123651505\n",
      "model.encoder.layer.9.attention.self.value.lora_B.backward_encoder.weight: 0.05540957301855087, Change: 0.05540957301855087\n",
      "model.encoder.layer.10.attention.self.query.lora_A.forward_encoder.weight: 1.1570794582366943, Change: -1.1444091796875e-05\n",
      "model.encoder.layer.10.attention.self.query.lora_A.backward_encoder.weight: 1.177735686302185, Change: -1.1682510375976562e-05\n",
      "model.encoder.layer.10.attention.self.query.lora_B.forward_encoder.weight: 0.05527803301811218, Change: 0.05527803301811218\n",
      "model.encoder.layer.10.attention.self.query.lora_B.backward_encoder.weight: 0.05535997822880745, Change: 0.05535997822880745\n",
      "model.encoder.layer.10.attention.self.value.lora_A.forward_encoder.weight: 1.1486293077468872, Change: -1.1682510375976562e-05\n",
      "model.encoder.layer.10.attention.self.value.lora_A.backward_encoder.weight: 1.159685730934143, Change: -1.1444091796875e-05\n",
      "model.encoder.layer.10.attention.self.value.lora_B.forward_encoder.weight: 0.05541186034679413, Change: 0.05541186034679413\n",
      "model.encoder.layer.10.attention.self.value.lora_B.backward_encoder.weight: 0.055419620126485825, Change: 0.055419620126485825\n",
      "model.encoder.layer.11.attention.self.query.lora_A.forward_encoder.weight: 1.1651560068130493, Change: -1.1682510375976562e-05\n",
      "model.encoder.layer.11.attention.self.query.lora_A.backward_encoder.weight: 1.153124451637268, Change: -1.1444091796875e-05\n",
      "model.encoder.layer.11.attention.self.query.lora_B.forward_encoder.weight: 0.0553310289978981, Change: 0.0553310289978981\n",
      "model.encoder.layer.11.attention.self.query.lora_B.backward_encoder.weight: 0.055363476276397705, Change: 0.055363476276397705\n",
      "model.encoder.layer.11.attention.self.value.lora_A.forward_encoder.weight: 1.162423849105835, Change: -1.1682510375976562e-05\n",
      "model.encoder.layer.11.attention.self.value.lora_A.backward_encoder.weight: 1.14215087890625, Change: -1.1563301086425781e-05\n",
      "model.encoder.layer.11.attention.self.value.lora_B.forward_encoder.weight: 0.055412571877241135, Change: 0.055412571877241135\n",
      "model.encoder.layer.11.attention.self.value.lora_B.backward_encoder.weight: 0.05539972707629204, Change: 0.05539972707629204\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, BitsAndBytesConfig\n",
    "from peft import LoraConfig\n",
    "\n",
    "# Assuming the BeliefStateTransformer class is already defined as provided earlier\n",
    "# Define dummy arguments for the model\n",
    "class Args:\n",
    "    model = \"bert-base-uncased\"  # Example model\n",
    "    load_in_4bit = False\n",
    "    use_flash = False\n",
    "    ptdtype = torch.float32\n",
    "    lora_r = 4\n",
    "    lora_alpha = 32\n",
    "    lora_dropout = 0.1\n",
    "    vocab_size = 30522  # Example vocab size\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Instantiate the model\n",
    "model = BeliefStateTransformer(args)\n",
    "model.train()\n",
    "\n",
    "# Create dummy input data\n",
    "batch_size = 2\n",
    "seq_length = 10\n",
    "dummy_x = torch.randint(0, args.vocab_size, (batch_size, seq_length))\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Define a GradScaler for mixed precision (optional)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=False)  # Disable for simplicity here\n",
    "\n",
    "# Run one forward and backward pass\n",
    "with torch.set_grad_enabled(True):\n",
    "    model.model.set_adapter(\"forward_encoder\")\n",
    "    forward_states = model.model(dummy_x).last_hidden_state\n",
    "    _f = forward_states.detach()\n",
    "    _f.requires_grad = True\n",
    "\n",
    "    # Compute backward states\n",
    "    model.model.set_adapter(\"backward_encoder\")\n",
    "    backward_input = torch.flip(dummy_x, dims=[1])\n",
    "    backward_states = model.model(backward_input).last_hidden_state\n",
    "    _b = backward_states.detach()\n",
    "    _b.requires_grad = True\n",
    "\n",
    "    # Compute the combined loss\n",
    "    loss = model.belief_state_objective(_f, _b, dummy_x)\n",
    "\n",
    "    # compute text head gradients over all prefix/suffix pairs.\n",
    "    scaled_loss = scaler.scale(loss)\n",
    "    print(\"Loss:\", scaled_loss.item())\n",
    "\n",
    "scaled_loss.backward()\n",
    "model.model.set_adapter(\"forward_encoder\")\n",
    "forward_states.backward(_f.grad)\n",
    "\n",
    "model.model.set_adapter(\"backward_encoder\")\n",
    "backward_states.backward(_b.grad)\n",
    "\n",
    "# model.model.set_adapter([\"forward_encoder\", \"backward_encoder\"])\n",
    "\n",
    "# Check if gradients exist for specific parameters\n",
    "print(\"\\nChecking gradients for model parameters...\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"{name}: {param.grad.norm().item()}\")\n",
    "    else:\n",
    "        print(f\"{name} has no gradient\")\n",
    "\n",
    "print(\"\\nParameter norms before optimizer step:\")\n",
    "param_norms_before = {}\n",
    "for name, param in model.named_parameters():\n",
    "    if \"forward_encoder\" in name or \"backward_encoder\" in name:\n",
    "        param_norms_before[name] = param.norm().item()\n",
    "        print(f\"{name}: {param.norm().item()}\")\n",
    "\n",
    "scaler.step(optimizer)\n",
    "scaler.update()\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "print(\"\\nParameter norms after optimizer step:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if name in param_norms_before:\n",
    "        print(f\"{name}: {param.norm().item()}, Change: {param.norm().item() - param_norms_before[name]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 142142324\n",
      "Trainable parameters: 32660084\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "input_text = \"This is a test.\"\n",
    "input_ids = tokenizer(input_text, add_special_tokens=True)[\"input_ids\"]\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "input_with_eos = input_ids + [eos_token_id]\n",
    "print(tokenizer.decode(input_with_eos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
