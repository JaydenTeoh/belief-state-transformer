{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Model\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "model = GPT2Model.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124439808"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params  = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaydenteoh/Library/Python/3.9/lib/python/site-packages/peft/tuners/lora/layer.py:1119: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class TiedTextHead(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, vocab_size, tied_weights=None):\n",
    "        super().__init__()\n",
    "        self.shared_mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # output is twice vocab size\n",
    "        # first half is for next token prediction: x_{t+1}\n",
    "        # second half is for previous token prediction: x_{t+k-1}\n",
    "        self.output_layer = nn.Linear(hidden_size, vocab_size * 2)\n",
    "\n",
    "    def forward(self, f, b):\n",
    "        combined = torch.cat([f, b], dim=-1)\n",
    "        shared_output = self.shared_mlp(combined)\n",
    "        logits = self.output_layer(shared_output)\n",
    "        return logits\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "            r=16, \n",
    "            lora_alpha=32, \n",
    "            lora_dropout=0.05, \n",
    "            # target_modules=[\"q_proj\", \"v_proj\"],  # apply lora to attention layers\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "\n",
    "# create separate forward and backward lora adapters\n",
    "model.add_adapter(lora_config, adapter_name=\"forward_encoder\")\n",
    "model.add_adapter(lora_config, adapter_name=\"backward_encoder\")\n",
    "\n",
    "# add tied text head for next and previous token predictions\n",
    "text_head = TiedTextHead(\n",
    "                    input_dim=model.config.hidden_size * 2,\n",
    "                    hidden_size=512, # TODO; allow this to be configurable\n",
    "                    vocab_size=50257,\n",
    "                    # tied_weights=self.model.transformer.wte.weight  # use input embeddings' weights\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "589824"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params  = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from utils.training_utils import accuracy\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class TiedTextHead(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, vocab_size, tied_weights=None):\n",
    "        super().__init__()\n",
    "        self.shared_mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        # output is twice vocab size\n",
    "        # first half is for next token prediction: x_{t+1}\n",
    "        # second half is for previous token prediction: x_{t+k-1}\n",
    "        self.output_layer = nn.Linear(hidden_size, vocab_size * 2)\n",
    "\n",
    "    def forward(self, f, b):\n",
    "        combined = torch.cat([f, b], dim=-1)\n",
    "        shared_output = self.shared_mlp(combined)\n",
    "        logits = self.output_layer(shared_output)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class BeliefStateTransformer(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "\n",
    "        if args.load_in_4bit:\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                                    load_in_4bit=True,\n",
    "                                    bnb_4bit_compute_dtype=args.ptdtype\n",
    "                                )\n",
    "        else:\n",
    "            # Assume default or no quantization\n",
    "            quantization_config = None\n",
    "\n",
    "        if args.use_flash:\n",
    "            attn_implementation = \"flash_attention_2\"\n",
    "        else:\n",
    "            attn_implementation = None\n",
    "        \n",
    "        # TODO: support more pretrained models\n",
    "        # import gpt2 with no specific head on top\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            args.model, \n",
    "            attn_implementation=attn_implementation,\n",
    "            torch_dtype=args.ptdtype,\n",
    "            quantization_config=quantization_config\n",
    "        )\n",
    "\n",
    "        # lora adapter config\n",
    "        lora_config = LoraConfig(\n",
    "            r=args.lora_r, \n",
    "            lora_alpha=args.lora_alpha, \n",
    "            lora_dropout=args.lora_dropout, \n",
    "            # target_modules=[\"q_proj\", \"v_proj\"],  # apply lora to attention layers\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "            fan_in_fan_out=True\n",
    "        )\n",
    "\n",
    "        # create separate forward and backward lora adapters\n",
    "        self.model.add_adapter(lora_config, adapter_name=\"forward_encoder\")\n",
    "        self.model.add_adapter(lora_config, adapter_name=\"backward_encoder\")\n",
    "\n",
    "        # enable gradient checkpointing to save memory during training\n",
    "        self.model.gradient_checkpointing_enable()\n",
    "\n",
    "        # add tied text head for next and previous token predictions\n",
    "        self.text_head = TiedTextHead(\n",
    "                            input_dim=self.model.config.hidden_size * 2,\n",
    "                            hidden_size=512, # TODO; allow this to be configurable\n",
    "                            vocab_size=args.vocab_size,\n",
    "                            # tied_weights=self.model.transformer.wte.weight  # use input embeddings' weights\n",
    "                        )\n",
    "\n",
    "    def forward(self, f, b):\n",
    "        # forward encoding\n",
    "        self.model.set_adapter(\"forward_encoder\")\n",
    "        forward_states = self.model(f).last_hidden_state  # get forward states\n",
    "\n",
    "        # backward encoding\n",
    "        self.model.set_adapter(\"backward_encoder\")\n",
    "        backward_input = torch.flip(b, dims=[1])  # reverse the input sequence\n",
    "        backward_states = self.model(backward_input).last_hidden_state\n",
    "        backward_states = torch.flip(backward_states, dims=[1])  # flip the backward states back\n",
    "\n",
    "        # Text head for next and previous token predictions\n",
    "        next_logits, prev_logits = self.text_head(forward_states, backward_states)\n",
    "        # acc, token_acc = accuracy(logits, targets)\n",
    "        # accs = {\"acc\": acc, \"token_acc\": token_acc}\n",
    "        return next_logits, prev_logits\n",
    "    \n",
    "    def belief_state_objective(self, all_f, all_b, x):\n",
    "        \"\"\"\n",
    "        Compute the belief state objective as described in the BST paper.\n",
    "        \"\"\"\n",
    "        bs, T = x.shape\n",
    "        forward_states = all_f\n",
    "        backward_states = all_b.flip(1)\n",
    "        # generate all valid combinations of forward and backward indices\n",
    "        ft = torch.arange(T, dtype=torch.int32)  # forward indices\n",
    "        bt = torch.arange(T, dtype=torch.int32)  # backward indices\n",
    "        combinations = torch.cartesian_prod(ft, bt) \n",
    "        combinations = combinations[(combinations[:, 1] - combinations[:, 0] >= 2)]  # filter valid pairs\n",
    "        fb_pairs = combinations[combinations[:, 1] < T]  # ensure backward indices are within range\n",
    "\n",
    "        # extract valid indices\n",
    "        f_idxs, b_idxs = fb_pairs[:, 0], fb_pairs[:, 1]\n",
    "        nt_idxs = (combinations[:, 0] + 1)  # indices for next token labels\n",
    "\n",
    "        # gather forward and backward features\n",
    "        f = forward_states[:, f_idxs]\n",
    "        b = backward_states[:, b_idxs]\n",
    "\n",
    "        # prepare labels\n",
    "        single_labels_f = x[:, nt_idxs].unsqueeze(2)  # labels for next-token prediction\n",
    "        single_labels_b = x[:, b_idxs].unsqueeze(2)   # labels for prev-token prediction\n",
    "        single_labels = torch.cat((single_labels_f, single_labels_b), dim=2)\n",
    "\n",
    "        # compute logits from the text head\n",
    "        logits = self.text_head(f, b)  # combine forward and backward states\n",
    "        fb_numpairs = fb_pairs.shape[0]  # no of valid forward-backward pairs\n",
    "\n",
    "        # reshape logits and labels for loss computation\n",
    "        logits = logits.reshape((bs, fb_numpairs, 2, -1))  # split into next and previous logits\n",
    "        logits = logits.reshape((bs * fb_numpairs * 2, -1))  # flatten for CEL\n",
    "        single_labels = single_labels.reshape((bs * fb_numpairs * 2))  # flatten labels\n",
    "\n",
    "        # compute the loss independently for next and previous token predictions\n",
    "        # this also sums the negative log likelihood for \n",
    "        # all next and previous token predictions together, aligning with the paper\n",
    "        loss = nn.CrossEntropyLoss()(logits, single_labels)\n",
    "        return loss\n",
    "    \n",
    "    def update(self, x, optimizer, scaler, targets=None):\n",
    "        \"\"\"\n",
    "        Efficient training for the BST model.\n",
    "        \"\"\"\n",
    "        # Compute forward states\n",
    "        self.model.set_adapter(\"forward_encoder\")\n",
    "        forward_states = self.model(x).last_hidden_state\n",
    "\n",
    "        # Compute backward states\n",
    "        self.model.set_adapter(\"backward_encoder\")\n",
    "        backward_input = torch.flip(x, dims=[1])\n",
    "        backward_states = self.model(backward_input).last_hidden_state\n",
    "\n",
    "        # Compute the combined loss\n",
    "        loss = self.belief_state_objective(forward_states, backward_states, x)\n",
    "        self.model.set_adapter([\"forward_encoder\", \"backward_encoder\"]) # IMPORTANT else gradient will only exist for backward_encoder\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=1):\n",
    "        \"\"\"\n",
    "        Generate text autoregressively using the forward model.\n",
    "\n",
    "        Args:\n",
    "            idx (torch.Tensor): Input tensor containing the tokenized prefix (batch_size, sequence_length).\n",
    "            max_length (int): Maximum length of the generated sequence.\n",
    "            temperature (float): Sampling temperature.\n",
    "            top_k (int): Top-k sampling.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Tokenized output sequence.\n",
    "        \"\"\"\n",
    "        bsz, prefix_len = idx.shape\n",
    "        device = idx.device\n",
    "\n",
    "        # Step 1: Precompute backward latent state for empty suffix B(∅)\n",
    "        empty_suffix = torch.zeros((bsz, 1), dtype=idx.dtype, device=device)\n",
    "        self.model.set_adapter(\"backward_encoder\")\n",
    "        backward_state = self.model(empty_suffix).last_hidden_state[:, 0:1, :]  # Fixed backward state\n",
    "\n",
    "        # Step 2: Initialize the generated sequence with the prefix\n",
    "        out = idx.clone()\n",
    "\n",
    "        for i in range(max_new_tokens):\n",
    "            # Step 3: Compute forward latent state for current prefix F(x1:t)\n",
    "            self.model.set_adapter(\"forward_encoder\")\n",
    "            forward_states = self.model(out).last_hidden_state\n",
    "\n",
    "            # Step 4: Compute logits for the next token prediction\n",
    "            logits = self.text_head(forward_states[:, -1:, :], backward_state)\n",
    "\n",
    "            next_logits = logits[:, 0, :self.text_head.output_layer.out_features // 2]\n",
    "\n",
    "            # Step 5: Apply temperature scaling\n",
    "            next_logits = next_logits / temperature\n",
    "\n",
    "            # Step 6: Apply top-k sampling if specified\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(next_logits, min(top_k, next_logits.size(-1)))\n",
    "                next_logits[next_logits < v[:, [-1]]] = -float('Inf')\n",
    "\n",
    "            # Step 7: Convert logits to probabilities and sample\n",
    "            probs = F.softmax(next_logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            # Step 8: Append sampled index to the running sequence\n",
    "            out = torch.cat((out, idx_next), dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    \n",
    "    # def _create_optimizer(self, lr, weight_decay):\n",
    "    #     return torch.optim.AdamW(\n",
    "    #         [\n",
    "    #             {\"params\": self.model.get_adapter(\"forward_encoder\").parameters(), \"lr\": lr},\n",
    "    #             {\"params\": self.model.get_adapter(\"backward_encoder\").parameters(), \"lr\": lr},\n",
    "    #             {\"params\": self.text_head.parameters(), \"lr\": lr}\n",
    "    #         ],\n",
    "    #         lr=lr,\n",
    "    #         weight_decay=weight_decay\n",
    "    #     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 10.324244499206543\n",
      "\n",
      "Checking gradients for model parameters...\n",
      "model.embeddings.word_embeddings.weight has no gradient\n",
      "model.embeddings.position_embeddings.weight has no gradient\n",
      "model.embeddings.token_type_embeddings.weight has no gradient\n",
      "model.embeddings.LayerNorm.weight has no gradient\n",
      "model.embeddings.LayerNorm.bias has no gradient\n",
      "model.encoder.layer.0.attention.self.query.base_layer.weight has no gradient\n",
      "model.encoder.layer.0.attention.self.query.base_layer.bias has no gradient\n",
      "model.encoder.layer.0.attention.self.query.lora_A.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.0.attention.self.query.lora_A.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.0.attention.self.query.lora_B.forward_encoder.weight: 0.009250209666788578\n",
      "model.encoder.layer.0.attention.self.query.lora_B.backward_encoder.weight: 0.011352716945111752\n",
      "model.encoder.layer.0.attention.self.key.weight has no gradient\n",
      "model.encoder.layer.0.attention.self.key.bias has no gradient\n",
      "model.encoder.layer.0.attention.self.value.base_layer.weight has no gradient\n",
      "model.encoder.layer.0.attention.self.value.base_layer.bias has no gradient\n",
      "model.encoder.layer.0.attention.self.value.lora_A.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.0.attention.self.value.lora_A.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.0.attention.self.value.lora_B.forward_encoder.weight: 0.02490842342376709\n",
      "model.encoder.layer.0.attention.self.value.lora_B.backward_encoder.weight: 0.03258025646209717\n",
      "model.encoder.layer.0.attention.output.dense.weight has no gradient\n",
      "model.encoder.layer.0.attention.output.dense.bias has no gradient\n",
      "model.encoder.layer.0.attention.output.LayerNorm.weight has no gradient\n",
      "model.encoder.layer.0.attention.output.LayerNorm.bias has no gradient\n",
      "model.encoder.layer.0.intermediate.dense.weight has no gradient\n",
      "model.encoder.layer.0.intermediate.dense.bias has no gradient\n",
      "model.encoder.layer.0.output.dense.weight has no gradient\n",
      "model.encoder.layer.0.output.dense.bias has no gradient\n",
      "model.encoder.layer.0.output.LayerNorm.weight has no gradient\n",
      "model.encoder.layer.0.output.LayerNorm.bias has no gradient\n",
      "model.encoder.layer.1.attention.self.query.base_layer.weight has no gradient\n",
      "model.encoder.layer.1.attention.self.query.base_layer.bias has no gradient\n",
      "model.encoder.layer.1.attention.self.query.lora_A.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.1.attention.self.query.lora_A.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.1.attention.self.query.lora_B.forward_encoder.weight: 0.009906348772346973\n",
      "model.encoder.layer.1.attention.self.query.lora_B.backward_encoder.weight: 0.011478212662041187\n",
      "model.encoder.layer.1.attention.self.key.weight has no gradient\n",
      "model.encoder.layer.1.attention.self.key.bias has no gradient\n",
      "model.encoder.layer.1.attention.self.value.base_layer.weight has no gradient\n",
      "model.encoder.layer.1.attention.self.value.base_layer.bias has no gradient\n",
      "model.encoder.layer.1.attention.self.value.lora_A.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.1.attention.self.value.lora_A.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.1.attention.self.value.lora_B.forward_encoder.weight: 0.03858561813831329\n",
      "model.encoder.layer.1.attention.self.value.lora_B.backward_encoder.weight: 0.03052099235355854\n",
      "model.encoder.layer.1.attention.output.dense.weight has no gradient\n",
      "model.encoder.layer.1.attention.output.dense.bias has no gradient\n",
      "model.encoder.layer.1.attention.output.LayerNorm.weight has no gradient\n",
      "model.encoder.layer.1.attention.output.LayerNorm.bias has no gradient\n",
      "model.encoder.layer.1.intermediate.dense.weight has no gradient\n",
      "model.encoder.layer.1.intermediate.dense.bias has no gradient\n",
      "model.encoder.layer.1.output.dense.weight has no gradient\n",
      "model.encoder.layer.1.output.dense.bias has no gradient\n",
      "model.encoder.layer.1.output.LayerNorm.weight has no gradient\n",
      "model.encoder.layer.1.output.LayerNorm.bias has no gradient\n",
      "model.encoder.layer.2.attention.self.query.base_layer.weight has no gradient\n",
      "model.encoder.layer.2.attention.self.query.base_layer.bias has no gradient\n",
      "model.encoder.layer.2.attention.self.query.lora_A.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.2.attention.self.query.lora_A.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.2.attention.self.query.lora_B.forward_encoder.weight: 0.009727207012474537\n",
      "model.encoder.layer.2.attention.self.query.lora_B.backward_encoder.weight: 0.009921579621732235\n",
      "model.encoder.layer.2.attention.self.key.weight has no gradient\n",
      "model.encoder.layer.2.attention.self.key.bias has no gradient\n",
      "model.encoder.layer.2.attention.self.value.base_layer.weight has no gradient\n",
      "model.encoder.layer.2.attention.self.value.base_layer.bias has no gradient\n",
      "model.encoder.layer.2.attention.self.value.lora_A.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.2.attention.self.value.lora_A.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.2.attention.self.value.lora_B.forward_encoder.weight: 0.0332564078271389\n",
      "model.encoder.layer.2.attention.self.value.lora_B.backward_encoder.weight: 0.045556556433439255\n",
      "model.encoder.layer.2.attention.output.dense.weight has no gradient\n",
      "model.encoder.layer.2.attention.output.dense.bias has no gradient\n",
      "model.encoder.layer.2.attention.output.LayerNorm.weight has no gradient\n",
      "model.encoder.layer.2.attention.output.LayerNorm.bias has no gradient\n",
      "model.encoder.layer.2.intermediate.dense.weight has no gradient\n",
      "model.encoder.layer.2.intermediate.dense.bias has no gradient\n",
      "model.encoder.layer.2.output.dense.weight has no gradient\n",
      "model.encoder.layer.2.output.dense.bias has no gradient\n",
      "model.encoder.layer.2.output.LayerNorm.weight has no gradient\n",
      "model.encoder.layer.2.output.LayerNorm.bias has no gradient\n",
      "model.encoder.layer.3.attention.self.query.base_layer.weight has no gradient\n",
      "model.encoder.layer.3.attention.self.query.base_layer.bias has no gradient\n",
      "model.encoder.layer.3.attention.self.query.lora_A.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.3.attention.self.query.lora_A.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.3.attention.self.query.lora_B.forward_encoder.weight: 0.007380272727459669\n",
      "model.encoder.layer.3.attention.self.query.lora_B.backward_encoder.weight: 0.010342128574848175\n",
      "model.encoder.layer.3.attention.self.key.weight has no gradient\n",
      "model.encoder.layer.3.attention.self.key.bias has no gradient\n",
      "model.encoder.layer.3.attention.self.value.base_layer.weight has no gradient\n",
      "model.encoder.layer.3.attention.self.value.base_layer.bias has no gradient\n",
      "model.encoder.layer.3.attention.self.value.lora_A.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.3.attention.self.value.lora_A.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.3.attention.self.value.lora_B.forward_encoder.weight: 0.033874545246362686\n",
      "model.encoder.layer.3.attention.self.value.lora_B.backward_encoder.weight: 0.036020487546920776\n",
      "model.encoder.layer.3.attention.output.dense.weight has no gradient\n",
      "model.encoder.layer.3.attention.output.dense.bias has no gradient\n",
      "model.encoder.layer.3.attention.output.LayerNorm.weight has no gradient\n",
      "model.encoder.layer.3.attention.output.LayerNorm.bias has no gradient\n",
      "model.encoder.layer.3.intermediate.dense.weight has no gradient\n",
      "model.encoder.layer.3.intermediate.dense.bias has no gradient\n",
      "model.encoder.layer.3.output.dense.weight has no gradient\n",
      "model.encoder.layer.3.output.dense.bias has no gradient\n",
      "model.encoder.layer.3.output.LayerNorm.weight has no gradient\n",
      "model.encoder.layer.3.output.LayerNorm.bias has no gradient\n",
      "model.encoder.layer.4.attention.self.query.base_layer.weight has no gradient\n",
      "model.encoder.layer.4.attention.self.query.base_layer.bias has no gradient\n",
      "model.encoder.layer.4.attention.self.query.lora_A.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.4.attention.self.query.lora_A.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.4.attention.self.query.lora_B.forward_encoder.weight: 0.009107079356908798\n",
      "model.encoder.layer.4.attention.self.query.lora_B.backward_encoder.weight: 0.010791550390422344\n",
      "model.encoder.layer.4.attention.self.key.weight has no gradient\n",
      "model.encoder.layer.4.attention.self.key.bias has no gradient\n",
      "model.encoder.layer.4.attention.self.value.base_layer.weight has no gradient\n",
      "model.encoder.layer.4.attention.self.value.base_layer.bias has no gradient\n",
      "model.encoder.layer.4.attention.self.value.lora_A.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.4.attention.self.value.lora_A.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.4.attention.self.value.lora_B.forward_encoder.weight: 0.04184376820921898\n",
      "model.encoder.layer.4.attention.self.value.lora_B.backward_encoder.weight: 0.03495224192738533\n",
      "model.encoder.layer.4.attention.output.dense.weight has no gradient\n",
      "model.encoder.layer.4.attention.output.dense.bias has no gradient\n",
      "model.encoder.layer.4.attention.output.LayerNorm.weight has no gradient\n",
      "model.encoder.layer.4.attention.output.LayerNorm.bias has no gradient\n",
      "model.encoder.layer.4.intermediate.dense.weight has no gradient\n",
      "model.encoder.layer.4.intermediate.dense.bias has no gradient\n",
      "model.encoder.layer.4.output.dense.weight has no gradient\n",
      "model.encoder.layer.4.output.dense.bias has no gradient\n",
      "model.encoder.layer.4.output.LayerNorm.weight has no gradient\n",
      "model.encoder.layer.4.output.LayerNorm.bias has no gradient\n",
      "model.encoder.layer.5.attention.self.query.base_layer.weight has no gradient\n",
      "model.encoder.layer.5.attention.self.query.base_layer.bias has no gradient\n",
      "model.encoder.layer.5.attention.self.query.lora_A.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.5.attention.self.query.lora_A.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.5.attention.self.query.lora_B.forward_encoder.weight: 0.008257041685283184\n",
      "model.encoder.layer.5.attention.self.query.lora_B.backward_encoder.weight: 0.007051101420074701\n",
      "model.encoder.layer.5.attention.self.key.weight has no gradient\n",
      "model.encoder.layer.5.attention.self.key.bias has no gradient\n",
      "model.encoder.layer.5.attention.self.value.base_layer.weight has no gradient\n",
      "model.encoder.layer.5.attention.self.value.base_layer.bias has no gradient\n",
      "model.encoder.layer.5.attention.self.value.lora_A.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.5.attention.self.value.lora_A.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.5.attention.self.value.lora_B.forward_encoder.weight: 0.058896999806165695\n",
      "model.encoder.layer.5.attention.self.value.lora_B.backward_encoder.weight: 0.02961609698832035\n",
      "model.encoder.layer.5.attention.output.dense.weight has no gradient\n",
      "model.encoder.layer.5.attention.output.dense.bias has no gradient\n",
      "model.encoder.layer.5.attention.output.LayerNorm.weight has no gradient\n",
      "model.encoder.layer.5.attention.output.LayerNorm.bias has no gradient\n",
      "model.encoder.layer.5.intermediate.dense.weight has no gradient\n",
      "model.encoder.layer.5.intermediate.dense.bias has no gradient\n",
      "model.encoder.layer.5.output.dense.weight has no gradient\n",
      "model.encoder.layer.5.output.dense.bias has no gradient\n",
      "model.encoder.layer.5.output.LayerNorm.weight has no gradient\n",
      "model.encoder.layer.5.output.LayerNorm.bias has no gradient\n",
      "model.encoder.layer.6.attention.self.query.base_layer.weight has no gradient\n",
      "model.encoder.layer.6.attention.self.query.base_layer.bias has no gradient\n",
      "model.encoder.layer.6.attention.self.query.lora_A.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.6.attention.self.query.lora_A.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.6.attention.self.query.lora_B.forward_encoder.weight: 0.007337822578847408\n",
      "model.encoder.layer.6.attention.self.query.lora_B.backward_encoder.weight: 0.0053101503290236\n",
      "model.encoder.layer.6.attention.self.key.weight has no gradient\n",
      "model.encoder.layer.6.attention.self.key.bias has no gradient\n",
      "model.encoder.layer.6.attention.self.value.base_layer.weight has no gradient\n",
      "model.encoder.layer.6.attention.self.value.base_layer.bias has no gradient\n",
      "model.encoder.layer.6.attention.self.value.lora_A.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.6.attention.self.value.lora_A.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.6.attention.self.value.lora_B.forward_encoder.weight: 0.038434017449617386\n",
      "model.encoder.layer.6.attention.self.value.lora_B.backward_encoder.weight: 0.03238340839743614\n",
      "model.encoder.layer.6.attention.output.dense.weight has no gradient\n",
      "model.encoder.layer.6.attention.output.dense.bias has no gradient\n",
      "model.encoder.layer.6.attention.output.LayerNorm.weight has no gradient\n",
      "model.encoder.layer.6.attention.output.LayerNorm.bias has no gradient\n",
      "model.encoder.layer.6.intermediate.dense.weight has no gradient\n",
      "model.encoder.layer.6.intermediate.dense.bias has no gradient\n",
      "model.encoder.layer.6.output.dense.weight has no gradient\n",
      "model.encoder.layer.6.output.dense.bias has no gradient\n",
      "model.encoder.layer.6.output.LayerNorm.weight has no gradient\n",
      "model.encoder.layer.6.output.LayerNorm.bias has no gradient\n",
      "model.encoder.layer.7.attention.self.query.base_layer.weight has no gradient\n",
      "model.encoder.layer.7.attention.self.query.base_layer.bias has no gradient\n",
      "model.encoder.layer.7.attention.self.query.lora_A.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.7.attention.self.query.lora_A.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.7.attention.self.query.lora_B.forward_encoder.weight: 0.0059789977967739105\n",
      "model.encoder.layer.7.attention.self.query.lora_B.backward_encoder.weight: 0.003908666782081127\n",
      "model.encoder.layer.7.attention.self.key.weight has no gradient\n",
      "model.encoder.layer.7.attention.self.key.bias has no gradient\n",
      "model.encoder.layer.7.attention.self.value.base_layer.weight has no gradient\n",
      "model.encoder.layer.7.attention.self.value.base_layer.bias has no gradient\n",
      "model.encoder.layer.7.attention.self.value.lora_A.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.7.attention.self.value.lora_A.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.7.attention.self.value.lora_B.forward_encoder.weight: 0.053631801158189774\n",
      "model.encoder.layer.7.attention.self.value.lora_B.backward_encoder.weight: 0.024038149043917656\n",
      "model.encoder.layer.7.attention.output.dense.weight has no gradient\n",
      "model.encoder.layer.7.attention.output.dense.bias has no gradient\n",
      "model.encoder.layer.7.attention.output.LayerNorm.weight has no gradient\n",
      "model.encoder.layer.7.attention.output.LayerNorm.bias has no gradient\n",
      "model.encoder.layer.7.intermediate.dense.weight has no gradient\n",
      "model.encoder.layer.7.intermediate.dense.bias has no gradient\n",
      "model.encoder.layer.7.output.dense.weight has no gradient\n",
      "model.encoder.layer.7.output.dense.bias has no gradient\n",
      "model.encoder.layer.7.output.LayerNorm.weight has no gradient\n",
      "model.encoder.layer.7.output.LayerNorm.bias has no gradient\n",
      "model.encoder.layer.8.attention.self.query.base_layer.weight has no gradient\n",
      "model.encoder.layer.8.attention.self.query.base_layer.bias has no gradient\n",
      "model.encoder.layer.8.attention.self.query.lora_A.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.8.attention.self.query.lora_A.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.8.attention.self.query.lora_B.forward_encoder.weight: 0.004456035792827606\n",
      "model.encoder.layer.8.attention.self.query.lora_B.backward_encoder.weight: 0.004933206830173731\n",
      "model.encoder.layer.8.attention.self.key.weight has no gradient\n",
      "model.encoder.layer.8.attention.self.key.bias has no gradient\n",
      "model.encoder.layer.8.attention.self.value.base_layer.weight has no gradient\n",
      "model.encoder.layer.8.attention.self.value.base_layer.bias has no gradient\n",
      "model.encoder.layer.8.attention.self.value.lora_A.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.8.attention.self.value.lora_A.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.8.attention.self.value.lora_B.forward_encoder.weight: 0.0622611939907074\n",
      "model.encoder.layer.8.attention.self.value.lora_B.backward_encoder.weight: 0.047865211963653564\n",
      "model.encoder.layer.8.attention.output.dense.weight has no gradient\n",
      "model.encoder.layer.8.attention.output.dense.bias has no gradient\n",
      "model.encoder.layer.8.attention.output.LayerNorm.weight has no gradient\n",
      "model.encoder.layer.8.attention.output.LayerNorm.bias has no gradient\n",
      "model.encoder.layer.8.intermediate.dense.weight has no gradient\n",
      "model.encoder.layer.8.intermediate.dense.bias has no gradient\n",
      "model.encoder.layer.8.output.dense.weight has no gradient\n",
      "model.encoder.layer.8.output.dense.bias has no gradient\n",
      "model.encoder.layer.8.output.LayerNorm.weight has no gradient\n",
      "model.encoder.layer.8.output.LayerNorm.bias has no gradient\n",
      "model.encoder.layer.9.attention.self.query.base_layer.weight has no gradient\n",
      "model.encoder.layer.9.attention.self.query.base_layer.bias has no gradient\n",
      "model.encoder.layer.9.attention.self.query.lora_A.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.9.attention.self.query.lora_A.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.9.attention.self.query.lora_B.forward_encoder.weight: 0.0037184706889092922\n",
      "model.encoder.layer.9.attention.self.query.lora_B.backward_encoder.weight: 0.0056427717208862305\n",
      "model.encoder.layer.9.attention.self.key.weight has no gradient\n",
      "model.encoder.layer.9.attention.self.key.bias has no gradient\n",
      "model.encoder.layer.9.attention.self.value.base_layer.weight has no gradient\n",
      "model.encoder.layer.9.attention.self.value.base_layer.bias has no gradient\n",
      "model.encoder.layer.9.attention.self.value.lora_A.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.9.attention.self.value.lora_A.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.9.attention.self.value.lora_B.forward_encoder.weight: 0.04323354363441467\n",
      "model.encoder.layer.9.attention.self.value.lora_B.backward_encoder.weight: 0.027409590780735016\n",
      "model.encoder.layer.9.attention.output.dense.weight has no gradient\n",
      "model.encoder.layer.9.attention.output.dense.bias has no gradient\n",
      "model.encoder.layer.9.attention.output.LayerNorm.weight has no gradient\n",
      "model.encoder.layer.9.attention.output.LayerNorm.bias has no gradient\n",
      "model.encoder.layer.9.intermediate.dense.weight has no gradient\n",
      "model.encoder.layer.9.intermediate.dense.bias has no gradient\n",
      "model.encoder.layer.9.output.dense.weight has no gradient\n",
      "model.encoder.layer.9.output.dense.bias has no gradient\n",
      "model.encoder.layer.9.output.LayerNorm.weight has no gradient\n",
      "model.encoder.layer.9.output.LayerNorm.bias has no gradient\n",
      "model.encoder.layer.10.attention.self.query.base_layer.weight has no gradient\n",
      "model.encoder.layer.10.attention.self.query.base_layer.bias has no gradient\n",
      "model.encoder.layer.10.attention.self.query.lora_A.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.10.attention.self.query.lora_A.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.10.attention.self.query.lora_B.forward_encoder.weight: 0.0031310219783335924\n",
      "model.encoder.layer.10.attention.self.query.lora_B.backward_encoder.weight: 0.003089725039899349\n",
      "model.encoder.layer.10.attention.self.key.weight has no gradient\n",
      "model.encoder.layer.10.attention.self.key.bias has no gradient\n",
      "model.encoder.layer.10.attention.self.value.base_layer.weight has no gradient\n",
      "model.encoder.layer.10.attention.self.value.base_layer.bias has no gradient\n",
      "model.encoder.layer.10.attention.self.value.lora_A.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.10.attention.self.value.lora_A.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.10.attention.self.value.lora_B.forward_encoder.weight: 0.02717135101556778\n",
      "model.encoder.layer.10.attention.self.value.lora_B.backward_encoder.weight: 0.047161541879177094\n",
      "model.encoder.layer.10.attention.output.dense.weight has no gradient\n",
      "model.encoder.layer.10.attention.output.dense.bias has no gradient\n",
      "model.encoder.layer.10.attention.output.LayerNorm.weight has no gradient\n",
      "model.encoder.layer.10.attention.output.LayerNorm.bias has no gradient\n",
      "model.encoder.layer.10.intermediate.dense.weight has no gradient\n",
      "model.encoder.layer.10.intermediate.dense.bias has no gradient\n",
      "model.encoder.layer.10.output.dense.weight has no gradient\n",
      "model.encoder.layer.10.output.dense.bias has no gradient\n",
      "model.encoder.layer.10.output.LayerNorm.weight has no gradient\n",
      "model.encoder.layer.10.output.LayerNorm.bias has no gradient\n",
      "model.encoder.layer.11.attention.self.query.base_layer.weight has no gradient\n",
      "model.encoder.layer.11.attention.self.query.base_layer.bias has no gradient\n",
      "model.encoder.layer.11.attention.self.query.lora_A.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.11.attention.self.query.lora_A.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.11.attention.self.query.lora_B.forward_encoder.weight: 0.005153050646185875\n",
      "model.encoder.layer.11.attention.self.query.lora_B.backward_encoder.weight: 0.003884637728333473\n",
      "model.encoder.layer.11.attention.self.key.weight has no gradient\n",
      "model.encoder.layer.11.attention.self.key.bias has no gradient\n",
      "model.encoder.layer.11.attention.self.value.base_layer.weight has no gradient\n",
      "model.encoder.layer.11.attention.self.value.base_layer.bias has no gradient\n",
      "model.encoder.layer.11.attention.self.value.lora_A.forward_encoder.weight: 0.0\n",
      "model.encoder.layer.11.attention.self.value.lora_A.backward_encoder.weight: 0.0\n",
      "model.encoder.layer.11.attention.self.value.lora_B.forward_encoder.weight: 0.03628015145659447\n",
      "model.encoder.layer.11.attention.self.value.lora_B.backward_encoder.weight: 0.04387716203927994\n",
      "model.encoder.layer.11.attention.output.dense.weight has no gradient\n",
      "model.encoder.layer.11.attention.output.dense.bias has no gradient\n",
      "model.encoder.layer.11.attention.output.LayerNorm.weight has no gradient\n",
      "model.encoder.layer.11.attention.output.LayerNorm.bias has no gradient\n",
      "model.encoder.layer.11.intermediate.dense.weight has no gradient\n",
      "model.encoder.layer.11.intermediate.dense.bias has no gradient\n",
      "model.encoder.layer.11.output.dense.weight has no gradient\n",
      "model.encoder.layer.11.output.dense.bias has no gradient\n",
      "model.encoder.layer.11.output.LayerNorm.weight has no gradient\n",
      "model.encoder.layer.11.output.LayerNorm.bias has no gradient\n",
      "model.pooler.dense.weight has no gradient\n",
      "model.pooler.dense.bias has no gradient\n",
      "text_head.shared_mlp.0.weight: 0.5089742541313171\n",
      "text_head.shared_mlp.0.bias: 0.03022015281021595\n",
      "text_head.shared_mlp.2.weight: 0.33308491110801697\n",
      "text_head.shared_mlp.2.bias: 0.07635629922151566\n",
      "text_head.output_layer.weight: 0.3560856580734253\n",
      "text_head.output_layer.bias: 0.19833596050739288\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, BitsAndBytesConfig\n",
    "from peft import LoraConfig\n",
    "\n",
    "# Assuming the BeliefStateTransformer class is already defined as provided earlier\n",
    "# Define dummy arguments for the model\n",
    "class Args:\n",
    "    model = \"bert-base-uncased\"  # Example model\n",
    "    load_in_4bit = False\n",
    "    use_flash = False\n",
    "    ptdtype = torch.float32\n",
    "    lora_r = 4\n",
    "    lora_alpha = 32\n",
    "    lora_dropout = 0.1\n",
    "    vocab_size = 30522  # Example vocab size\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Instantiate the model\n",
    "model = BeliefStateTransformer(args)\n",
    "model.train()\n",
    "\n",
    "# Create dummy input data\n",
    "batch_size = 2\n",
    "seq_length = 10\n",
    "dummy_x = torch.randint(0, args.vocab_size, (batch_size, seq_length))\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Define a GradScaler for mixed precision (optional)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=False)  # Disable for simplicity here\n",
    "\n",
    "# Run one forward and backward pass\n",
    "with torch.set_grad_enabled(True):\n",
    "    model.model.set_adapter(\"forward_encoder\")\n",
    "    forward_states = model.model(dummy_x).last_hidden_state\n",
    "    _f = forward_states.detach()\n",
    "    _f.requires_grad = True\n",
    "\n",
    "    # Compute backward states\n",
    "    model.model.set_adapter(\"backward_encoder\")\n",
    "    backward_input = torch.flip(dummy_x, dims=[1])\n",
    "    backward_states = model.model(backward_input).last_hidden_state\n",
    "    _b = backward_states.detach()\n",
    "    _b.requires_grad = True\n",
    "\n",
    "    # Compute the combined loss\n",
    "    loss = model.belief_state_objective(_f, _b, dummy_x)\n",
    "\n",
    "    # compute text head gradients over all prefix/suffix pairs.\n",
    "    scaled_loss = scaler.scale(loss)\n",
    "    print(\"Loss:\", scaled_loss.item())\n",
    "\n",
    "scaled_loss.backward()\n",
    "model.model.set_adapter(\"forward_encoder\")\n",
    "forward_states.backward(_f.grad)\n",
    "\n",
    "model.model.set_adapter(\"backward_encoder\")\n",
    "backward_states.backward(_b.grad)\n",
    "\n",
    "# Check if gradients exist for specific parameters\n",
    "print(\"\\nChecking gradients for model parameters...\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"{name}: {param.grad.norm().item()}\")\n",
    "    else:\n",
    "        print(f\"{name} has no gradient\")\n",
    "\n",
    "\n",
    "scaler.step(optimizer)\n",
    "scaler.update()\n",
    "optimizer.zero_grad(set_to_none=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 142142324\n",
      "Trainable parameters: 32660084\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
